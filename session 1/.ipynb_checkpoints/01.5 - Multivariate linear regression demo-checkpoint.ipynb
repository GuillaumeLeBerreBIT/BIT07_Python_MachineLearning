{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt                        # To create plots\n",
    "import numpy as np                                     # To perform calculations quickly\n",
    "import pandas as pd                                    # To load in and manipulate data\n",
    "from sklearn import linear_model                       # Linear model\n",
    "from sklearn.model_selection import train_test_split   # Split up the data in a train and test set\n",
    "from ipywidgets import interact,widgets                # For interactive execution of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multivariate linear regression, the model is determined by a trainingset with multiple features. \n",
    "\n",
    "For example: the blood pressure is determined by both age and weight\n",
    "\n",
    "Instead of the usual y=ax+b we now will get:\n",
    "\n",
    "$h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "data = pd.read_csv(\"data/bloodpressure.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(data.age,data.weight,data.blood_pressure,c=data.weight)\n",
    "# Project the points on the walls\n",
    "ax.scatter3D(data.age.min(),data.weight,data.blood_pressure,c='pink')\n",
    "ax.scatter3D(data.age,data.weight.min(),data.blood_pressure,c='pink')\n",
    "ax.scatter3D(data.age,data.weight,data.blood_pressure.min(),c='pink')\n",
    "ax.view_init(30, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take everything but also doop blood pressure (left weight & age)\n",
    "features=data.drop('blood_pressure',axis=1) \n",
    "# Blood pressure column still present but now frame with age & weight\n",
    "target=data.blood_pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lregmodel=linear_model.LinearRegression()\n",
    "\n",
    "lregmodel.fit(features,target)\n",
    "# Theta 1 & Theta 2\n",
    "print(f\"Coefficients: {lregmodel.coef_}\")\n",
    "# Theta 0 -- > Stays the same (thats why in equation do not have to multiplicate)\n",
    "print(f\"Intercept: {lregmodel.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation that best fits our points in the dataset is:\n",
    "\n",
    "$blood\\_pressure = 0.85708344 * age + 0.71619724 * weight + 34.62648847714202$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the blood pressure of a person thats 55 years old and weighs 80kg\n",
    "\n",
    "blood_pressure = 0.85708344 * 55 + 0.71619724 * 80 + 34.62648847714202\n",
    "print(blood_pressure)\n",
    "\n",
    "#or\n",
    "\n",
    "print(lregmodel.predict([[55,80]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CRIM - per capita crime rate by town\n",
    "* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS - proportion of non-retail business acres per town.\n",
    "* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "* NOX - nitric oxides concentration (parts per 10 million)\n",
    "* RM - average number of rooms per dwelling\n",
    "* AGE - proportion of owner-occupied units built prior to 1940\n",
    "* DIS - weighted distances to five Boston employment centres\n",
    "* RAD - index of accessibility to radial highways\n",
    "* TAX - full-value property-tax rate per  $10,000 \n",
    "* PTRATIO - pupil-teacher ratio by town\n",
    "* B -  1000(𝐵𝑘−0.63)2  where  𝐵𝑘  is the proportion of blacks by town\n",
    "* LSTAT - % lower status of the population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/boston_housing.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics for the table\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove uninformative columns\n",
    "## CHAS is always 0\n",
    "# inplace = True -- > The actual data will be removed. \n",
    "# axis = 1 -- > Column\n",
    "# axis = 0 -- > Row\n",
    "data.drop(\"CHAS\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "Outliers are values that are very different from normal values. In some cases you want to remove these values. In other cases however, you want to keep these. For example when you want to detect these outliers.\n",
    "\n",
    "Outliers are often described differently. In this case we will only keep values that lay within 3 standard deviations from the mean. We will not take the categorical or binary values into consideration.\n",
    "\n",
    "We can use the zscore function from scipy. This will return the z-scores for each value. A z of 1 means the value is 1 standard deviation higher mean. A z of -1 means that its one standard deviation lower than the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "from scipy import stats\n",
    "\n",
    "# Get Z-scores for specific columns\n",
    "zscores=stats.zscore(data)\n",
    "# Absolute values\n",
    "abs_zcores=abs(zscores)\n",
    "# Is Z-score < 3\n",
    "filtered_zscore=abs_zcores<3\n",
    "# Get a True if all collumns of a row have z-scores <3, else: False\n",
    "filtered = filtered_zscore.all(axis=1)\n",
    "# Only keep these rows\n",
    "data_preprocessed = data[filtered]\n",
    "\n",
    "data_preprocessed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Visualize correlations\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = data_preprocessed.corr()\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pairplots same representation as the heatmap\n",
    "sns.pairplot(data_preprocessed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and targets\n",
    "X = data_preprocessed.drop(\"Price\",axis=1)\n",
    "y = data_preprocessed[\"Price\"]\n",
    "# or\n",
    "# y = data_preprocessed.Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test set\n",
    "Instead of training or model on the entire dataset, we will split up the data in a training set and a test set.\n",
    "\n",
    "This will allow us to see if our model is also good on unseen data.\n",
    "\n",
    "If our model has a low error on our training set, but a high error on our test set, then our model is **overfitted**\n",
    "\n",
    "If our model has a high error on our training and test set, then our model is **underfitted** or our model is **not complex enough** for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../images/under-overfit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in train and test set\n",
    "## train_test_split takes X, y, a test (or train size) and a random_state (=seed)\n",
    "## This size can be the fraction (0.33) = 30% or an absolute number (1000)\n",
    "## The seed allows you to always have the exact same split (useful for testing)\n",
    "## This will take 0.33% of the rows of the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty model\n",
    "lregmodel=linear_model.LinearRegression()\n",
    "\n",
    "# Fit our training data on the model\n",
    "lregmodel.fit(X_train,y_train)\n",
    "\n",
    "# Resulting coefficients and intercept\n",
    "print(f\"Coefficients: {(coef := lregmodel.coef_)}\")          # The walrus operator (:=) is used to assign and use a variable at the same time\n",
    "print(f\"Intercept: {(intercept := lregmodel.intercept_)}\")   # This way, we can use coef and intercept in the next part\n",
    "\n",
    "# Results in:\n",
    "\n",
    "## Get the column names of X_train\n",
    "columns = X_train.columns\n",
    "\n",
    "## Combine the coefficient with the correct column name:\n",
    "## Zip function will combine 2 lists/arrays as following: [a,b,c] and [1,2,3] => [(a,1),(b,2),(c,3)]\n",
    "\n",
    "coefpart = \" + \".join([f\"{round(coefficient,3)} * {feature}\" for feature,coefficient in zip(columns,coef)])\n",
    "\n",
    "print(f\"\\nPredicted price = {coefpart} + {intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "house = np.array([0.04,80,4.95,0.41,6.63,23.40,5.12,4,245,19.20,396.90,4.70])\n",
    "\n",
    "# The model expects an array of arrays to test: np.array(np.array(house1),np.array(house2),...)\n",
    "# Now we just have an array of one house: np.array(house1). We need to reshape this data to be in\n",
    "# the form: np.array(np.array(house1)). We do this with .reshape(1,-1)\n",
    "price = lregmodel.predict(house.reshape(1,-1))\n",
    "print(price)\n",
    "\n",
    "print(\"Actual price: 585.50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lregmodel.predict(X_test.values[0:2]))\n",
    "print(y_test[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "We can test our model by determining the error between the test set results and predictions\n",
    "\n",
    "Different ways of determining the error exist:\n",
    "* Mean absolute error (MAE): the average of the absolute values of the real values $y_i$ and the predicted values $\\hat{y}_i$\n",
    " * $MAE = \\frac{1}{n}\\sum\\limits_{i=1}^n|y_i-\\hat{y}_i|$\n",
    "* Mean squared error (MSE): the averages of the squared values of the real values $y_i$ and the predicted values\n",
    " * $MSE = \\frac{1}{n}\\sum\\limits_{i=1}^n(y_i-\\hat{y}_i)^2$\n",
    "* Coefficient of determination (R²): a proportion of the variability based on the features. Perfect prediction: R²=1. Negative R²: the mean of the targets is better than the model\n",
    " * $R^2 = 1-\\frac{\\sum\\limits_{i=1}^n(y_i-\\hat{y}_i)^2}{\\sum\\limits_{i=1}^n(y_i-\\bar{y}_i)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementations of these tests:\n",
    "def MAE(y_test,y_pred):\n",
    "    if (length:=len(y_test))!=len(y_test):\n",
    "        print(\"Lenght of y_test != length of y_pred!\")\n",
    "        return None\n",
    "    return 1/length*sum(abs(y_test-y_pred))\n",
    "\n",
    "def MSE(y_test,y_pred):\n",
    "    if (length:=len(y_test))!=len(y_test):\n",
    "        print(\"Lenght of y_test != length of y_pred!\")\n",
    "        return None\n",
    "    return 1/length*sum((y_test-y_pred)**2)\n",
    "\n",
    "def R2(y_test,y_pred):\n",
    "    if (length:=len(y_test))!=len(y_test):\n",
    "        print(\"Lenght of y_test != length of y_pred!\")\n",
    "        return None\n",
    "    return 1 - (np.sum((y_test-y_pred)**2)/np.sum((y_test-np.mean(y_test))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our model\n",
    "y_pred = lregmodel.predict(X_test)\n",
    "\n",
    "# Self written functions\n",
    "print(f\"MAE: {MAE(y_test,y_pred)}\")\n",
    "print(f\"MSE: {MSE(y_test,y_pred)}\")\n",
    "print(f\"R2 : {R2(y_test,y_pred)}\")\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "# Functions from sklearn (the minimal difference is due to rounding errors)\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "print(f\"MAE: {mean_absolute_error(y_test,y_pred)}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test,y_pred)}\")\n",
    "print(f\"R2 : {r2_score(y_test,y_pred)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Alternatively, R2 can also be calculated with the model:\n",
    "print(f\"R2 : {lregmodel.score(X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Normalization causes your dataset to be at the same scale.\n",
    "\n",
    "In our example: AGE goes from 2.9 to 100 whilst NOX goes from 0.385 to 0.871\n",
    "\n",
    "If features are on a different scale, gradient descent will occur slower.\n",
    "\n",
    "Types of Normalization:\n",
    "* Min-max scaling: scales between 0 and 1. Not good with outliers, good for gaussian distributions with small variance\n",
    " * $x_{si}=\\frac{x_i-Min(x)}{Max(x)-min(x)}$\n",
    "* Standard scaling: mean=0 and standard deviation = 1, will cause the values be around 0. Isn't affected that much by outliers. \n",
    " * $x_{si}=\\frac{x_i-mean(x)}{stdev(x)}$\n",
    "* Robust scaling: looks like Min-max scaling, but uses interquartiles instead of range (so not limited between 0 and 1). Isn't affected that much by outliers.\n",
    " * $x_{si}=\\frac{x_i-Q_2(x)}{Q_3(x)-Q_1(x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,RobustScaler\n",
    "\n",
    "# All scalers follow the same syntax:\n",
    "# First fit the scaler on X_train: scaler.fit(X_train)                     No fitting on the test set!\n",
    "# Next transform X_train and X_test using the scaler: \n",
    "# X_train=scaler.transform(X_train) and X_test=scaler.transform(X_test)\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "#scaler=StandardScaler()\n",
    "#scaler=RobustScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "# Now it has an average of 0 and some bit lower and higher -- > .transform\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Can see that the values are now pushed towards zero.\n",
    "print(X_train.values[0])\n",
    "print(X_train_scaled[0])\n",
    "#Create model >> Train model >> Predict test set\n",
    "lregmodel=linear_model.LinearRegression()\n",
    "lregmodel.fit(X_train_scaled,y_train)\n",
    "y_pred = lregmodel.predict(X_test_scaled)\n",
    "\n",
    "print()\n",
    "print(f\"MAE: {mean_absolute_error(y_test,y_pred)}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test,y_pred)}\")\n",
    "print(f\"R2 : {r2_score(y_test,y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature expansion\n",
    "Sometimes adding new features can add extra information.\n",
    "\n",
    "Examples:\n",
    "* Calculate the surface based on width and height\n",
    "* New features that were measured\n",
    "* Higher order features => Non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example higher order features\n",
    "data.plot(x='LSTAT',y='Price',kind=\"scatter\")\n",
    "\n",
    "X=data.LSTAT.values.reshape(-1,1)\n",
    "y=data.Price.values.reshape(-1,1)\n",
    "\n",
    "lregmodel=linear_model.LinearRegression()\n",
    "lregmodel.fit(X,y)\n",
    "plt.plot(X,lregmodel.predict(X), color='orange',linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.LSTAT.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a second order of LSTAT\n",
    "data.plot(x='LSTAT',y='Price',kind=\"scatter\")\n",
    "\n",
    "data.insert(data.columns.size-1,'LSTAT^2',data.LSTAT**2)\n",
    "\n",
    "X=data[[\"LSTAT\",\"LSTAT^2\"]].values\n",
    "y=data.Price.values.reshape(-1,1)\n",
    "\n",
    "lregmodel=linear_model.LinearRegression()\n",
    "lregmodel.fit(X,y)\n",
    "\n",
    "# Calculate a value from 0 to 40\n",
    "newX= np.array([range(40),[x**2 for x in range(40)]])\n",
    "\n",
    "plt.plot(newX[0],lregmodel.predict(newX.T), color='orange',linewidth=3)\n",
    "#plt.scatter(newX[0],lregmodel.predict(newX.T),color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a third order of LSTAT\n",
    "data.plot(x='LSTAT',y='Price',kind=\"scatter\")\n",
    "\n",
    "data.insert(data.columns.size-1,'LSTAT^3',data.LSTAT**3)\n",
    "\n",
    "X=data[[\"LSTAT\",\"LSTAT^2\",\"LSTAT^3\"]].values\n",
    "y=data.Price.values.reshape(-1,1)\n",
    "\n",
    "lregmodel=linear_model.LinearRegression()\n",
    "lregmodel.fit(X,y)\n",
    "\n",
    "# Calculate a value from 0 to 40\n",
    "newX= np.array([range(40),[x**2 for x in range(40)],[x**3 for x in range(40)]])\n",
    "\n",
    "plt.plot(newX[0],lregmodel.predict(newX.T), color='orange',linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a fourth order of LSTAT\n",
    "data.plot(x='LSTAT',y='Price',kind=\"scatter\")\n",
    "\n",
    "data.insert(data.columns.size-1,'LSTAT^4',data.LSTAT**4)\n",
    "\n",
    "X=data[[\"LSTAT\",\"LSTAT^2\",\"LSTAT^3\",\"LSTAT^4\"]].values\n",
    "y=data.Price.values.reshape(-1,1)\n",
    "\n",
    "lregmodel=linear_model.LinearRegression()\n",
    "lregmodel.fit(X,y)\n",
    "\n",
    "# Calculate a value from 0 to 40\n",
    "newX= np.array([range(40),[x**2 for x in range(40)],[x**3 for x in range(40)],[x**4 for x in range(40)]])\n",
    "\n",
    "plt.plot(newX[0],lregmodel.predict(newX.T), color='orange',linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e549542cb0d4317a2d0e3d85eac5600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=12, description='order', max=20, min=1), IntSlider(value=100, descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's overfit\n",
    "# Add 12 orders of LSTAT based on the first 100 data points\n",
    "# (Let's actually add up to 20, since then we can try this interactively and change the number of orders between 1 and 20)\n",
    "data = pd.read_csv(\"data/boston_housing.csv\")\n",
    "for i in range(2,21):\n",
    "    data.insert(data.columns.size-1,f\"LSTAT^{i}\",data.LSTAT**i)\n",
    "\n",
    "@interact(order=(1,20,1),data_points=(3,data.shape[0],1),)\n",
    "def plotHigherorder(order=12,data_points=100):\n",
    "    data.iloc[0:data_points,:].plot(x='LSTAT',y='Price',kind=\"scatter\")\n",
    "\n",
    "    X=data.iloc[0:data_points,12:12+order]\n",
    "    minX=int(X.LSTAT.min())\n",
    "    maxX=int(X.LSTAT.max())\n",
    "    y=data.Price.values[0:data_points].reshape(-1,1)\n",
    "\n",
    "    lregmodel=linear_model.LinearRegression()\n",
    "    lregmodel.fit(X,y)\n",
    "\n",
    "    # Calculate a value from the minimum value of the subset to the maximum value of the subset\n",
    "    newX= np.array([[x**i for x in range(minX,maxX+1)]for i in range(1,order+1)])\n",
    "\n",
    "    plt.plot(newX[0],lregmodel.predict(newX.T), color='orange',linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "A method to find a good balance between under- and overfitting.\n",
    "\n",
    "This can be accomplished by adding an extra term to our loss function: $R(\\theta)$ the regularization term \n",
    "\n",
    "$J(\\theta) = \\frac{1}{2m}\\sum \\limits _{i=1} ^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^{2} + R(\\theta)$\n",
    "\n",
    "$R(\\theta)$ is an extra penalty that punishes the use of higher order features unless they decrease the entire cost of the loss function\n",
    "\n",
    "$\\theta$ is the used feature. (e.g. Price or LSTAT) ($\\theta_0$ (= the intercept) is often not regularized)\n",
    "\n",
    "There are two major forms of regularization:\n",
    "* L1 (Lasso regression)\n",
    "* L2 (Ridge regression)\n",
    "\n",
    "#### L1 regression:\n",
    "Uses the sum of the absolute values of $\\theta$\n",
    "* $J_{L1} = J + \\lambda_1\\sum\\limits_{j=1}^m\\lvert{\\theta_j}\\rvert$\n",
    "* $J_{L1} = \\sum \\limits _{i=1} ^{n}(target^{(i)} - output^{(i)})^2 + \\lambda_1\\sum\\limits_{j=1}^m\\lvert{\\theta_j}\\rvert $\n",
    "\n",
    "![L1-regression.png](../images/L1-regression.png)\n",
    "\n",
    "\n",
    "#### L2 regression:\n",
    "Uses the sum of the squared values of $\\theta$\n",
    "* $J_{L2} = J + \\lambda_2\\sum\\limits_{j=1}^m\\theta_j^2$\n",
    "* $J_{L2} = \\sum \\limits _{i=1} ^{n}(target^{(i)} - output^{(i)})^2 + \\lambda_2\\sum\\limits_{j=1}^m\\theta_j^2 $\n",
    "\n",
    "$\\lambda$ is the tuning parameter (hyper parameter)\n",
    "* $\\lambda = 0 =>$ no regularization\n",
    "* $\\lambda = \\infty => \\theta = 0 $ (no orders are used ever)\n",
    "* $0 < \\lambda < \\infty => $ possibility to tune the regularization \n",
    "\n",
    "![L2-regression.png](../images/L2-regression.png)\n",
    "\n",
    "A smaller $\\lambda$ results in a lower bias and higher variance = overfitting\n",
    "\n",
    "A bigger $\\lambda$ results in a higher bias and lower variance = underfitting\n",
    "\n",
    "#### Implementation in sklearn\n",
    "Instead of $\\lambda$ we use the \"alpha\" parameter in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 (Lasso)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7de2a8b008404897504416b1751889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatText(value=0.1, description='alpha'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 (Ridge)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bbf0ad322241d184ed29e015185e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatText(value=0.1, description='alpha'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso,Ridge\n",
    "\n",
    "higher_order_features=10\n",
    "data_points=50\n",
    "\n",
    "data = pd.read_csv(\"data/boston_housing.csv\")\n",
    "for i in range(2,higher_order_features+1):\n",
    "    data.insert(data.columns.size-1,f\"LSTAT^{i}\",data.LSTAT**i)\n",
    "\n",
    "X=data.iloc[0:data_points,12:12+higher_order_features]\n",
    "minX=int(X.LSTAT.min())\n",
    "maxX=int(X.LSTAT.max())\n",
    "y=data.Price.values[0:data_points].reshape(-1,1)\n",
    "\n",
    "# Calculate a value from 3 to 30\n",
    "newX= np.array([[x**i for x in range(minX,maxX+1)]for i in range(1,higher_order_features+1)])\n",
    "\n",
    "# L1 linear regression = Lasso\n",
    "print(\"L1 (Lasso)\")\n",
    "@interact(alpha=widgets.FloatText(value=0.1))\n",
    "def Lasso_model(alpha=0.1):\n",
    "    Lregmodel = Lasso(alpha=alpha, fit_intercept=True)\n",
    "    Lregmodel.fit(X, y)\n",
    "    score = Lregmodel.score(X,y)\n",
    "    print(f\"Score: {score}\")\n",
    "    #print(newX)\n",
    "    data.iloc[0:data_points,:].plot(x='LSTAT',y='Price',kind=\"scatter\")\n",
    "    plt.plot(newX[0],Lregmodel.predict(newX.T), color='orange',linewidth=3)\n",
    "\n",
    "# L2 linear regression = Ridge\n",
    "print(\"L2 (Ridge)\")\n",
    "@interact(alpha=widgets.FloatText(value=0.1))\n",
    "def Ridge_model(alpha=0.1):\n",
    "    Rregmodel = Ridge(alpha=alpha, fit_intercept=True)\n",
    "    Rregmodel.fit(X, y)\n",
    "    score = Rregmodel.score(X,y)\n",
    "    print(f\"Score: {score}\")\n",
    "    data.iloc[0:data_points,:].plot(x='LSTAT',y='Price',kind=\"scatter\")\n",
    "    plt.plot(newX[0],Rregmodel.predict(newX.T), color='orange',linewidth=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
