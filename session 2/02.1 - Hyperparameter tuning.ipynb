{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact,widgets                # For interactive execution of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "data = pd.read_csv(\"data/heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first lines\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.366337</td>\n",
       "      <td>0.683168</td>\n",
       "      <td>0.966997</td>\n",
       "      <td>131.623762</td>\n",
       "      <td>246.264026</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>149.646865</td>\n",
       "      <td>0.326733</td>\n",
       "      <td>1.039604</td>\n",
       "      <td>1.399340</td>\n",
       "      <td>0.729373</td>\n",
       "      <td>2.313531</td>\n",
       "      <td>0.544554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.082101</td>\n",
       "      <td>0.466011</td>\n",
       "      <td>1.032052</td>\n",
       "      <td>17.538143</td>\n",
       "      <td>51.830751</td>\n",
       "      <td>0.356198</td>\n",
       "      <td>0.525860</td>\n",
       "      <td>22.905161</td>\n",
       "      <td>0.469794</td>\n",
       "      <td>1.161075</td>\n",
       "      <td>0.616226</td>\n",
       "      <td>1.022606</td>\n",
       "      <td>0.612277</td>\n",
       "      <td>0.498835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>274.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp    trestbps        chol         fbs  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean    54.366337    0.683168    0.966997  131.623762  246.264026    0.148515   \n",
       "std      9.082101    0.466011    1.032052   17.538143   51.830751    0.356198   \n",
       "min     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n",
       "25%     47.500000    0.000000    0.000000  120.000000  211.000000    0.000000   \n",
       "50%     55.000000    1.000000    1.000000  130.000000  240.000000    0.000000   \n",
       "75%     61.000000    1.000000    2.000000  140.000000  274.500000    0.000000   \n",
       "max     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n",
       "\n",
       "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean     0.528053  149.646865    0.326733    1.039604    1.399340    0.729373   \n",
       "std      0.525860   22.905161    0.469794    1.161075    0.616226    1.022606   \n",
       "min      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
       "50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n",
       "75%      1.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
       "max      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n",
       "\n",
       "             thal      target  \n",
       "count  303.000000  303.000000  \n",
       "mean     2.313531    0.544554  \n",
       "std      0.612277    0.498835  \n",
       "min      0.000000    0.000000  \n",
       "25%      2.000000    0.000000  \n",
       "50%      2.000000    1.000000  \n",
       "75%      3.000000    1.000000  \n",
       "max      3.000000    1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and targets\n",
    "features = data.drop(\"target\",axis=1)\n",
    "target = data[\"target\"]\n",
    "\n",
    "# Split into train and test set (50 test samples)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,target,test_size=50,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# The base model we want to test\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "# All the parameters we want to test\n",
    "parameters = [{\"C\":np.linspace(0.01,20,10), # linspace will evenly space values between a start and a stop. \n",
    "                                            # In this case 10 values evenly spaced between 0.01 and 20\n",
    "              \"penalty\":[\"l1\",\"l2\"], # All penalties we want to check, we could also add `None` \n",
    "              \"solver\":[\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]}] # The solvers to use \n",
    "                                                                            # (different underlying algorithms, see docs)\n",
    "\n",
    "# Constructing the grid search \"model\"\n",
    "grid_search = GridSearchCV(estimator=model,             # The base model\n",
    "                           cv=5,                        # Number of cross fold validations\n",
    "                           param_grid=parameters,       # Different parameters to test\n",
    "                           n_jobs=-1,                   # Number of threads to use (-1: all)\n",
    "                           verbose=5,                   # How much information do we want to show? See docs for more info\n",
    "                           scoring=\"balanced_accuracy\") # What score will the model be evaluated on?\n",
    "                                                        # See: https://scikit-learn.org/stable/modules/classes.html?highlight=metric#module-sklearn.metrics\n",
    "# Fit the training data\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the score on test set (best model of our grid search)\n",
    "print(f\"Score of best model: {grid_search.score(X_test,y_test)}\")\n",
    "print(f\"Score of best model: {grid_search.score(X_train,y_train)}\")\n",
    "# Predict values for test set for best model\n",
    "y_pred=grid_search.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# What where the best parameters for out model?\n",
    "print(\"\\nBest parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=16.57624815929893, penalty=l1, solver=liblinear;, score=0.804 total time=   0.0s\n",
      "[CV 5/5] END C=16.57624815929893, penalty=l1, solver=liblinear;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=6.696959776184739, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 3/5] END C=6.696959776184739, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.2s\n",
      "[CV 5/5] END C=6.696959776184739, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END C=4.3771312907109365, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END C=4.3771312907109365, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END C=17.11195324635432, penalty=l1, solver=saga;, score=0.627 total time=   0.1s\n",
      "[CV 5/5] END C=17.11195324635432, penalty=l1, solver=saga;, score=0.720 total time=   0.1s\n",
      "[CV 4/5] END C=19.891280986251424, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 2/5] END C=1.8918277714760812, penalty=l2, solver=newton-cg;, score=0.784 total time=   0.0s\n",
      "[CV 5/5] END C=1.8918277714760812, penalty=l2, solver=newton-cg;, score=0.820 total time=   0.0s\n",
      "[CV 3/5] END C=11.923273173257758, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 2/5] END C=18.587894435680695, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.2s\n",
      "[CV 3/5] END C=18.587894435680695, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 2/5] END C=1.304425613478527, penalty=l2, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 3/5] END C=1.304425613478527, penalty=l2, solver=liblinear;, score=0.922 total time=   0.0s\n",
      "[CV 4/5] END C=1.304425613478527, penalty=l2, solver=liblinear;, score=0.840 total time=   0.0s\n",
      "[CV 5/5] END C=1.304425613478527, penalty=l2, solver=liblinear;, score=0.820 total time=   0.0s\n",
      "[CV 1/5] END C=5.632160122179949, penalty=l2, solver=newton-cg;, score=0.804 total time=   0.0s\n",
      "[CV 2/5] END C=5.632160122179949, penalty=l2, solver=newton-cg;, score=0.784 total time=   0.0s\n",
      "[CV 5/5] END C=5.632160122179949, penalty=l2, solver=newton-cg;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=17.05870378738271, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=17.05870378738271, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=17.05870378738271, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=17.05870378738271, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=17.05870378738271, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=17.33122048599132, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=14.169117203731817, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 3/5] END C=9.384440469021026, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=9.384440469021026, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 4/5] END C=8.85575300990523, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=8.85575300990523, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=4.5642994283146185, penalty=l2, solver=sag;, score=0.745 total time=   0.0s\n",
      "[CV 2/5] END C=4.5642994283146185, penalty=l2, solver=sag;, score=0.686 total time=   0.0s\n",
      "[CV 1/5] END C=17.037268605608823, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=17.037268605608823, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=17.037268605608823, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=17.037268605608823, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=16.525500103407385, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=12.236972016276992, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=12.236972016276992, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=12.236972016276992, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=12.236972016276992, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=12.236972016276992, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=4.567096247551362, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=4.567096247551362, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=19.99305026289231, penalty=l1, solver=liblinear;, score=0.804 total time=   0.0s\n",
      "[CV 3/5] END C=19.99305026289231, penalty=l1, solver=liblinear;, score=0.863 total time=   0.0s\n",
      "[CV 4/5] END C=19.99305026289231, penalty=l1, solver=liblinear;, score=0.840 total time=   0.0s\n",
      "[CV 5/5] END C=19.99305026289231, penalty=l1, solver=liblinear;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=16.505654789032715, penalty=l2, solver=liblinear;, score=0.804 total time=   0.0s\n",
      "[CV 2/5] END C=16.505654789032715, penalty=l2, solver=liblinear;, score=0.824 total time=   0.0s\n",
      "[CV 3/5] END C=16.505654789032715, penalty=l2, solver=liblinear;, score=0.882 total time=   0.0s\n",
      "[CV 4/5] END C=16.505654789032715, penalty=l2, solver=liblinear;, score=0.840 total time=   0.0s\n",
      "[CV 3/5] END C=16.88248895539271, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=16.88248895539271, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=16.88248895539271, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=6.775796998944104, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=6.775796998944104, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=6.775796998944104, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=6.775796998944104, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=6.775796998944104, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=10.861744579869308, penalty=l2, solver=sag;, score=0.860 total time=   0.1s\n",
      "[CV 5/5] END C=10.861744579869308, penalty=l2, solver=sag;, score=0.760 total time=   0.0s\n",
      "[CV 1/5] END C=10.938072958829789, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=10.938072958829789, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=10.938072958829789, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=10.938072958829789, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=10.938072958829789, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=13.680377805855178, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 1/5] END C=4.005996850386512, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=4.005996850386512, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=4.005996850386512, penalty=l2, solver=lbfgs;, score=0.882 total time=   0.1s\n",
      "[CV 4/5] END C=4.005996850386512, penalty=l2, solver=lbfgs;, score=0.820 total time=   0.1s\n",
      "[CV 5/5] END C=4.005996850386512, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=3.004207812075933, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=3.004207812075933, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=3.004207812075933, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 1/5] END C=1.5862469106974586, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 2/5] END C=1.5862469106974586, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 3/5] END C=1.5862469106974586, penalty=l1, solver=liblinear;, score=0.922 total time=   0.0s\n",
      "[CV 4/5] END C=1.5862469106974586, penalty=l1, solver=liblinear;, score=0.840 total time=   0.0s\n",
      "[CV 5/5] END C=1.5862469106974586, penalty=l1, solver=liblinear;, score=0.820 total time=   0.0s\n",
      "[CV 1/5] END C=14.547489759726094, penalty=l2, solver=sag;, score=0.745 total time=   0.0s\n",
      "[CV 2/5] END C=14.547489759726094, penalty=l2, solver=sag;, score=0.686 total time=   0.0s\n",
      "[CV 3/5] END C=14.547489759726094, penalty=l2, solver=sag;, score=0.824 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=16.57624815929893, penalty=l1, solver=liblinear;, score=0.840 total time=   0.0s\n",
      "[CV 3/5] END C=19.891280986251424, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 1/5] END C=1.8918277714760812, penalty=l2, solver=newton-cg;, score=0.804 total time=   0.0s\n",
      "[CV 4/5] END C=1.8918277714760812, penalty=l2, solver=newton-cg;, score=0.840 total time=   0.0s\n",
      "[CV 2/5] END C=11.923273173257758, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 1/5] END C=18.587894435680695, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.2s\n",
      "[CV 3/5] END C=18.132939496227237, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=18.132939496227237, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 1/5] END C=17.33122048599132, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=17.33122048599132, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=14.169117203731817, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=14.169117203731817, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 2/5] END C=13.015607957424212, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=13.015607957424212, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=13.015607957424212, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=13.015607957424212, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=18.438988658314855, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=18.438988658314855, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=18.438988658314855, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=18.438988658314855, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 4/5] END C=15.209443461645757, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=15.209443461645757, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=4.247343189737974, penalty=l2, solver=newton-cg;, score=0.804 total time=   0.0s\n",
      "[CV 2/5] END C=4.247343189737974, penalty=l2, solver=newton-cg;, score=0.784 total time=   0.0s\n",
      "[CV 3/5] END C=4.247343189737974, penalty=l2, solver=newton-cg;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=4.247343189737974, penalty=l2, solver=newton-cg;, score=0.820 total time=   0.0s\n",
      "[CV 5/5] END C=4.247343189737974, penalty=l2, solver=newton-cg;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=16.83432977001864, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=4.567096247551362, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=4.567096247551362, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=4.567096247551362, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=12.12039608475501, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=12.12039608475501, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 3/5] END C=12.12039608475501, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=12.12039608475501, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=12.12039608475501, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END C=16.505654789032715, penalty=l2, solver=liblinear;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=19.79117269765309, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=19.79117269765309, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 3/5] END C=19.79117269765309, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=19.79117269765309, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=19.79117269765309, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=16.88248895539271, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=16.88248895539271, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 3/5] END C=18.674730059207832, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=18.674730059207832, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=18.674730059207832, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=4.6463387256909, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=4.6463387256909, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=4.6463387256909, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=4.6463387256909, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=4.6463387256909, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=3.004207812075933, penalty=l2, solver=lbfgs;, score=0.820 total time=   0.1s\n",
      "[CV 5/5] END C=3.004207812075933, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=19.151704815620192, penalty=l2, solver=newton-cg;, score=0.824 total time=   0.0s\n",
      "[CV 2/5] END C=19.151704815620192, penalty=l2, solver=newton-cg;, score=0.804 total time=   0.0s\n",
      "[CV 3/5] END C=19.151704815620192, penalty=l2, solver=newton-cg;, score=0.863 total time=   0.0s\n",
      "[CV 4/5] END C=19.151704815620192, penalty=l2, solver=newton-cg;, score=0.840 total time=   0.0s\n",
      "[CV 5/5] END C=19.151704815620192, penalty=l2, solver=newton-cg;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=0.6473226989587733, penalty=l2, solver=sag;, score=0.745 total time=   0.0s\n",
      "[CV 3/5] END C=9.63472988864879, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=9.63472988864879, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=9.63472988864879, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=18.87004849838875, penalty=l2, solver=newton-cg;, score=0.824 total time=   0.0s\n",
      "[CV 2/5] END C=18.87004849838875, penalty=l2, solver=newton-cg;, score=0.804 total time=   0.0s\n",
      "[CV 3/5] END C=18.87004849838875, penalty=l2, solver=newton-cg;, score=0.863 total time=   0.0s\n",
      "[CV 4/5] END C=18.87004849838875, penalty=l2, solver=newton-cg;, score=0.840 total time=   0.0s\n",
      "[CV 5/5] END C=18.87004849838875, penalty=l2, solver=newton-cg;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=5.55503416398898, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=5.55503416398898, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=5.55503416398898, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=5.55503416398898, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=5.55503416398898, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=10.345656865784862, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=10.345656865784862, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=10.345656865784862, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=16.955228057425483, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 3/5] END C=16.955228057425483, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=16.955228057425483, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=16.955228057425483, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=0.18226436600703777, penalty=l1, solver=saga;, score=0.725 total time=   0.0s\n",
      "[CV 2/5] END C=0.18226436600703777, penalty=l1, solver=saga;, score=0.627 total time=   0.1s\n",
      "[CV 3/5] END C=0.18226436600703777, penalty=l1, solver=saga;, score=0.765 total time=   0.0s\n",
      "[CV 4/5] END C=0.18226436600703777, penalty=l1, solver=saga;, score=0.820 total time=   0.0s\n",
      "[CV 5/5] END C=18.43267706895152, penalty=l2, solver=newton-cg;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=4.781548015823157, penalty=l2, solver=sag;, score=0.745 total time=   0.0s\n",
      "[CV 2/5] END C=4.781548015823157, penalty=l2, solver=sag;, score=0.686 total time=   0.0s\n",
      "[CV 3/5] END C=4.781548015823157, penalty=l2, solver=sag;, score=0.824 total time=   0.0s\n",
      "[CV 3/5] END C=16.57624815929893, penalty=l1, solver=liblinear;, score=0.863 total time=   0.0s\n",
      "[CV 3/5] END C=4.3771312907109365, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=17.11195324635432, penalty=l1, solver=saga;, score=0.840 total time=   0.1s\n",
      "[CV 2/5] END C=19.891280986251424, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END C=19.891280986251424, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=11.923273173257758, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 5/5] END C=11.923273173257758, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END C=18.587894435680695, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=18.587894435680695, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END C=18.132939496227237, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=1.304425613478527, penalty=l2, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 3/5] END C=5.632160122179949, penalty=l2, solver=newton-cg;, score=0.863 total time=   0.0s\n",
      "[CV 4/5] END C=5.632160122179949, penalty=l2, solver=newton-cg;, score=0.840 total time=   0.0s\n",
      "[CV 3/5] END C=17.33122048599132, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=17.33122048599132, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=14.169117203731817, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=14.169117203731817, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END C=9.384440469021026, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=13.015607957424212, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=18.438988658314855, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=8.85575300990523, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=8.85575300990523, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=8.85575300990523, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=4.5642994283146185, penalty=l2, solver=sag;, score=0.824 total time=   0.0s\n",
      "[CV 4/5] END C=4.5642994283146185, penalty=l2, solver=sag;, score=0.860 total time=   0.0s\n",
      "[CV 5/5] END C=4.5642994283146185, penalty=l2, solver=sag;, score=0.760 total time=   0.0s\n",
      "[CV 1/5] END C=7.6763197328865616, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 5/5] END C=17.037268605608823, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=15.209443461645757, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=15.209443461645757, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=15.209443461645757, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 1/5] END C=15.84774118907426, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=15.84774118907426, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 3/5] END C=15.84774118907426, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=15.84774118907426, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=15.84774118907426, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=11.209307923536059, penalty=l2, solver=sag;, score=0.745 total time=   0.0s\n",
      "[CV 2/5] END C=11.209307923536059, penalty=l2, solver=sag;, score=0.686 total time=   0.0s\n",
      "[CV 3/5] END C=11.209307923536059, penalty=l2, solver=sag;, score=0.824 total time=   0.0s\n",
      "[CV 1/5] END C=16.018425926811616, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=16.018425926811616, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 3/5] END C=16.018425926811616, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=16.018425926811616, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=16.018425926811616, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=10.861744579869308, penalty=l2, solver=sag;, score=0.745 total time=   0.0s\n",
      "[CV 2/5] END C=10.861744579869308, penalty=l2, solver=sag;, score=0.686 total time=   0.0s\n",
      "[CV 3/5] END C=10.861744579869308, penalty=l2, solver=sag;, score=0.824 total time=   0.0s\n",
      "[CV 5/5] END C=10.760730652858626, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=15.422213359349948, penalty=l2, solver=sag;, score=0.745 total time=   0.0s\n",
      "[CV 2/5] END C=15.422213359349948, penalty=l2, solver=sag;, score=0.686 total time=   0.0s\n",
      "[CV 3/5] END C=15.422213359349948, penalty=l2, solver=sag;, score=0.824 total time=   0.0s\n",
      "[CV 4/5] END C=15.422213359349948, penalty=l2, solver=sag;, score=0.860 total time=   0.0s\n",
      "[CV 5/5] END C=15.422213359349948, penalty=l2, solver=sag;, score=0.760 total time=   0.0s\n",
      "[CV 1/5] END C=18.674730059207832, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=18.674730059207832, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=0.6473226989587733, penalty=l2, solver=sag;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END C=0.6473226989587733, penalty=l2, solver=sag;, score=0.804 total time=   0.0s\n",
      "[CV 4/5] END C=0.6473226989587733, penalty=l2, solver=sag;, score=0.860 total time=   0.0s\n",
      "[CV 5/5] END C=0.6473226989587733, penalty=l2, solver=sag;, score=0.760 total time=   0.0s\n",
      "[CV 1/5] END C=17.837365969057696, penalty=l1, solver=saga;, score=0.706 total time=   0.0s\n",
      "[CV 2/5] END C=17.837365969057696, penalty=l1, solver=saga;, score=0.627 total time=   0.0s\n",
      "[CV 3/5] END C=17.837365969057696, penalty=l1, solver=saga;, score=0.765 total time=   0.0s\n",
      "[CV 4/5] END C=17.837365969057696, penalty=l1, solver=saga;, score=0.840 total time=   0.0s\n",
      "[CV 4/5] END C=10.345656865784862, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=10.345656865784862, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=15.714256714058912, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=15.714256714058912, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=15.714256714058912, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=15.714256714058912, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=15.714256714058912, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=16.955228057425483, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 5/5] END C=0.18226436600703777, penalty=l1, solver=saga;, score=0.720 total time=   0.0s\n",
      "[CV 1/5] END C=10.233798571503739, penalty=l1, solver=liblinear;, score=0.804 total time=   0.0s\n",
      "[CV 2/5] END C=10.233798571503739, penalty=l1, solver=liblinear;, score=0.804 total time=   0.0s\n",
      "[CV 3/5] END C=10.233798571503739, penalty=l1, solver=liblinear;, score=0.863 total time=   0.0s\n",
      "[CV 4/5] END C=10.233798571503739, penalty=l1, solver=liblinear;, score=0.840 total time=   0.0s\n",
      "[CV 5/5] END C=10.233798571503739, penalty=l1, solver=liblinear;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=9.817945907042022, penalty=l2, solver=liblinear;, score=0.804 total time=   0.0s\n",
      "[CV 2/5] END C=9.817945907042022, penalty=l2, solver=liblinear;, score=0.824 total time=   0.0s\n",
      "[CV 3/5] END C=9.817945907042022, penalty=l2, solver=liblinear;, score=0.882 total time=   0.0s\n",
      "[CV 4/5] END C=9.817945907042022, penalty=l2, solver=liblinear;, score=0.840 total time=   0.0s\n",
      "[CV 5/5] END C=9.817945907042022, penalty=l2, solver=liblinear;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=16.431493878412258, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=16.431493878412258, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 3/5] END C=16.431493878412258, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=16.431493878412258, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=16.431493878412258, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END C=17.2941310535965, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=16.57624815929893, penalty=l1, solver=liblinear;, score=0.804 total time=   0.0s\n",
      "[CV 2/5] END C=6.696959776184739, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END C=6.696959776184739, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.2s\n",
      "[CV 1/5] END C=4.3771312907109365, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 4/5] END C=4.3771312907109365, penalty=l2, solver=lbfgs;, score=0.820 total time=   0.1s\n",
      "[CV 1/5] END C=17.11195324635432, penalty=l1, solver=saga;, score=0.706 total time=   0.1s\n",
      "[CV 3/5] END C=17.11195324635432, penalty=l1, solver=saga;, score=0.765 total time=   0.0s\n",
      "[CV 1/5] END C=19.891280986251424, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.2s\n",
      "[CV 3/5] END C=1.8918277714760812, penalty=l2, solver=newton-cg;, score=0.882 total time=   0.1s\n",
      "[CV 4/5] END C=11.923273173257758, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 1/5] END C=18.132939496227237, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.2s\n",
      "[CV 2/5] END C=18.132939496227237, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END C=9.384440469021026, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=9.384440469021026, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.2s\n",
      "[CV 2/5] END C=7.6763197328865616, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=7.6763197328865616, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=7.6763197328865616, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=7.6763197328865616, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END C=16.83432977001864, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=16.83432977001864, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=16.83432977001864, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=16.83432977001864, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=16.525500103407385, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=16.525500103407385, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=16.525500103407385, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=16.525500103407385, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 4/5] END C=11.209307923536059, penalty=l2, solver=sag;, score=0.860 total time=   0.0s\n",
      "[CV 5/5] END C=11.209307923536059, penalty=l2, solver=sag;, score=0.760 total time=   0.0s\n",
      "[CV 1/5] END C=17.390736979864318, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=17.390736979864318, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 3/5] END C=17.390736979864318, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=17.390736979864318, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=17.390736979864318, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=19.99305026289231, penalty=l1, solver=liblinear;, score=0.804 total time=   0.0s\n",
      "[CV 2/5] END C=13.680377805855178, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=13.680377805855178, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=13.680377805855178, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=13.680377805855178, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=10.760730652858626, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=10.760730652858626, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=10.760730652858626, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=10.760730652858626, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=17.837365969057696, penalty=l1, solver=saga;, score=0.720 total time=   0.1s\n",
      "[CV 1/5] END C=1.0443198846018376, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=1.0443198846018376, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=1.0443198846018376, penalty=l2, solver=lbfgs;, score=0.882 total time=   0.1s\n",
      "[CV 4/5] END C=1.0443198846018376, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=1.0443198846018376, penalty=l2, solver=lbfgs;, score=0.820 total time=   0.1s\n",
      "[CV 1/5] END C=9.63472988864879, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=9.63472988864879, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=14.547489759726094, penalty=l2, solver=sag;, score=0.860 total time=   0.0s\n",
      "[CV 5/5] END C=14.547489759726094, penalty=l2, solver=sag;, score=0.760 total time=   0.0s\n",
      "[CV 1/5] END C=16.746283431444795, penalty=l2, solver=saga;, score=0.706 total time=   0.0s\n",
      "[CV 2/5] END C=16.746283431444795, penalty=l2, solver=saga;, score=0.627 total time=   0.0s\n",
      "[CV 3/5] END C=16.746283431444795, penalty=l2, solver=saga;, score=0.765 total time=   0.0s\n",
      "[CV 4/5] END C=16.746283431444795, penalty=l2, solver=saga;, score=0.840 total time=   0.0s\n",
      "[CV 5/5] END C=16.746283431444795, penalty=l2, solver=saga;, score=0.720 total time=   0.1s\n",
      "[CV 1/5] END C=19.68332293007883, penalty=l1, solver=saga;, score=0.706 total time=   0.0s\n",
      "[CV 1/5] END C=8.055031503944786, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=8.055031503944786, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=8.055031503944786, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=8.055031503944786, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=8.055031503944786, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=5.295562742413681, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=5.295562742413681, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=5.295562742413681, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 2/5] END C=13.92397193076634, penalty=l2, solver=sag;, score=0.686 total time=   0.0s\n",
      "[CV 3/5] END C=13.92397193076634, penalty=l2, solver=sag;, score=0.824 total time=   0.0s\n",
      "[CV 4/5] END C=13.92397193076634, penalty=l2, solver=sag;, score=0.860 total time=   0.0s\n",
      "[CV 5/5] END C=13.92397193076634, penalty=l2, solver=sag;, score=0.760 total time=   0.0s\n",
      "[CV 1/5] END C=18.770150033231992, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=18.770150033231992, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=18.770150033231992, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=18.770150033231992, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=2.61490894253778, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=2.61490894253778, penalty=l2, solver=lbfgs;, score=0.820 total time=   0.1s\n",
      "[CV 5/5] END C=2.61490894253778, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=7.515227032464336, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=7.515227032464336, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.2s\n",
      "[CV 3/5] END C=7.515227032464336, penalty=l2, solver=lbfgs;, score=0.882 total time=   0.1s\n",
      "[CV 4/5] END C=7.515227032464336, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=7.515227032464336, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END C=14.294897319340754, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=14.294897319340754, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=14.294897319340754, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=18.026252988717314, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=18.026252988717314, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=18.026252988717314, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "80 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.82211765 0.81819608 0.81419608 0.73160784 0.82211765 0.82611765\n",
      " 0.82603922 0.81819608 0.82211765 0.83003922 0.81819608        nan\n",
      "        nan 0.82211765 0.82211765        nan 0.82211765        nan\n",
      " 0.77498039 0.82211765 0.82211765 0.82211765 0.81419608        nan\n",
      " 0.82211765 0.82211765        nan 0.82603922 0.82603922 0.77498039\n",
      " 0.82603922 0.82211765 0.82996078 0.82603922 0.82603922        nan\n",
      " 0.82603922 0.77498039 0.81819608 0.82211765 0.82211765 0.77498039\n",
      " 0.82211765        nan 0.81811765 0.81419608 0.82603922 0.76713725\n",
      " 0.73160784 0.82611765        nan 0.82603922        nan        nan\n",
      "        nan 0.82603922 0.73152941 0.82211765 0.82996078 0.82603922\n",
      " 0.83003922 0.77498039 0.73160784 0.73160784 0.82603922 0.77498039\n",
      " 0.82603922 0.81811765        nan 0.81419608 0.81419608 0.77498039\n",
      "        nan 0.82996078 0.81419608 0.82211765 0.81819608 0.81419608\n",
      "        nan 0.82211765 0.82611765 0.82211765 0.82211765 0.82211765\n",
      " 0.82211765 0.73160784 0.81811765 0.73160784 0.73160784 0.82996078\n",
      " 0.82211765 0.81819608 0.84188235 0.82211765 0.81811765 0.81819608\n",
      " 0.82211765        nan 0.82603922 0.82603922]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=LogisticRegression(max_iter=1000),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions=[{&#x27;C&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7fb364378640&gt;,\n",
       "                                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                                         &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;liblinear&#x27;,\n",
       "                                                    &#x27;sag&#x27;, &#x27;saga&#x27;]},\n",
       "                                        {&#x27;C&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7fb3643dc550&gt;,\n",
       "                                         &#x27;penalty&#x27;: [&#x27;l2&#x27;],\n",
       "                                         &#x27;solver&#x27;: [&#x27;lbfgs&#x27;]}],\n",
       "                   verbose=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=LogisticRegression(max_iter=1000),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions=[{&#x27;C&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7fb364378640&gt;,\n",
       "                                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                                         &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;liblinear&#x27;,\n",
       "                                                    &#x27;sag&#x27;, &#x27;saga&#x27;]},\n",
       "                                        {&#x27;C&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7fb3643dc550&gt;,\n",
       "                                         &#x27;penalty&#x27;: [&#x27;l2&#x27;],\n",
       "                                         &#x27;solver&#x27;: [&#x27;lbfgs&#x27;]}],\n",
       "                   verbose=5)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=LogisticRegression(max_iter=1000),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions=[{'C': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7fb364378640>,\n",
       "                                         'penalty': ['l1', 'l2'],\n",
       "                                         'solver': ['newton-cg', 'liblinear',\n",
       "                                                    'sag', 'saga']},\n",
       "                                        {'C': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7fb3643dc550>,\n",
       "                                         'penalty': ['l2'],\n",
       "                                         'solver': ['lbfgs']}],\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# The base models\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# You can also evaluate different things for different models.\n",
    "# You can add in seperate parameters that can be combined like this: paramaters=[{Set 1 of params},{Set 2 of params}]\n",
    "# Note that in grid search we got a lot of errors, mainly complaining that \"lbfgs\" solver cannot use L1 regularisation.\n",
    "# Here we removed this solver from the first set and added in another set of params, with only lbfgs and L2, but still \n",
    "# changing the C value!\n",
    "parameters = [\n",
    "              #Set 1\n",
    "              {\"C\":uniform(0.01,20),# Uniform: a uniform distribution of values between a minimum and maximum. Needed for\n",
    "                                    # the random search\n",
    "              \"penalty\":[\"l1\",\"l2\"],\n",
    "              \"solver\":[\"newton-cg\", \"liblinear\", \"sag\", \"saga\"]},\n",
    "    \n",
    "              #Set 2\n",
    "              {\"C\":uniform(0.01,20),\n",
    "              \"penalty\":[\"l2\"],\n",
    "              \"solver\":[\"lbfgs\"]}\n",
    "             ]\n",
    "\n",
    "# Number of models (before CV) to evaluate\n",
    "n_iter_search = 100\n",
    "\n",
    "# The random search \"model\", similar to the grid search\n",
    "random_search = RandomizedSearchCV(estimator=model,\n",
    "                                   cv=5,\n",
    "                                   param_distributions=parameters,\n",
    "                                   n_iter=n_iter_search,\n",
    "                                   n_jobs=-1,\n",
    "                                   verbose=5)\n",
    "# Fit on the training set\n",
    "random_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of best model: 0.84\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.80      0.83        25\n",
      "           1       0.81      0.88      0.85        25\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.84      0.84      0.84        50\n",
      "weighted avg       0.84      0.84      0.84        50\n",
      "\n",
      "\n",
      "Best parameters:\n",
      "{'C': 3.010625846470093, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "LogisticRegression(C=3.010625846470093, max_iter=1000, solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "# Print score of the best model on test set\n",
    "print(f\"Score of best model: {random_search.score(X_test,y_test)}\")\n",
    "\n",
    "# Predict the classes for the test set\n",
    "y_pred=random_search.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# Show the best classifier parameters\n",
    "print(\"\\nBest parameters:\")\n",
    "print(random_search.best_params_)\n",
    "print(random_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in /home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages (from scikit-optimize) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages (from scikit-optimize) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages (from scikit-optimize) (1.24.2)\n",
      "Requirement already satisfied: pyaml>=16.9 in /home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages (from scikit-optimize) (21.10.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages (from scikit-optimize) (1.10.0)\n",
      "Requirement already satisfied: PyYAML in /home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/guest/miniconda3/envs/bit07/lib/python3.9/site-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "builtins.int = int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 23\u001b[0m\n\u001b[1;32m     16\u001b[0m bayes_search \u001b[38;5;241m=\u001b[39m BayesSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     17\u001b[0m                                    cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     18\u001b[0m                                    search_spaces\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m     19\u001b[0m                                    n_iter\u001b[38;5;241m=\u001b[39mn_iter_search,\n\u001b[1;32m     20\u001b[0m                                    n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     21\u001b[0m                                    verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Fit on the training data\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mbayes_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/skopt/searchcv.py:466\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[0;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs)\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_train_score:\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/skopt/searchcv.py:512\u001b[0m, in \u001b[0;36mBayesSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n_iter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     n_points_adjusted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_iter, n_points)\n\u001b[0;32m--> 512\u001b[0m     optim_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_points_adjusted\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     n_iter \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n_points\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/skopt/searchcv.py:400\u001b[0m, in \u001b[0;36mBayesSearchCV._step\u001b[0;34m(self, search_space, optimizer, evaluate_candidates, n_points)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate n_jobs parameters and evaluate them in parallel.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# get parameter values to evaluate\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# convert parameters to python native types\u001b[39;00m\n\u001b[1;32m    403\u001b[0m params \u001b[38;5;241m=\u001b[39m [[np\u001b[38;5;241m.\u001b[39marray(v)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m p] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params]\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:395\u001b[0m, in \u001b[0;36mOptimizer.ask\u001b[0;34m(self, n_points, strategy)\u001b[0m\n\u001b[1;32m    393\u001b[0m X \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_points):\n\u001b[0;32m--> 395\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    398\u001b[0m     ti_available \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macq_func \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(opt\u001b[38;5;241m.\u001b[39myi) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:367\u001b[0m, in \u001b[0;36mOptimizer.ask\u001b[0;34m(self, n_points, strategy)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Query point or multiple points at which objective should be evaluated.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03mn_points : int or None, default: None\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m \n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_points \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m supported_strategies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl_min\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl_max\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(n_points, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m n_points \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:434\u001b[0m, in \u001b[0;36mOptimizer._ask\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_initial_points \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_estimator_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;66;03m# this will not make a copy of `self.rng` and hence keep advancing\u001b[39;00m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;66;03m# our random state.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 434\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;66;03m# The samples are evaluated starting form initial_samples[0]\u001b[39;00m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_samples[\n\u001b[1;32m    438\u001b[0m             \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_initial_points]\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/skopt/space/space.py:900\u001b[0m, in \u001b[0;36mSpace.rvs\u001b[0;34m(self, n_samples, random_state)\u001b[0m\n\u001b[1;32m    897\u001b[0m columns \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdimensions:\n\u001b[0;32m--> 900\u001b[0m     columns\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# Transpose\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _transpose_list_array(columns)\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/skopt/space/space.py:698\u001b[0m, in \u001b[0;36mCategorical.rvs\u001b[0;34m(self, n_samples, random_state)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minverse_transform([(choices)])\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalize\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories[c] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m choices]\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/skopt/space/space.py:685\u001b[0m, in \u001b[0;36mCategorical.inverse_transform\u001b[0;34m(self, Xt)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Inverse transform samples from the warped space back into the\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03m   original space.\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;66;03m# The concatenation of all transformed dimensions makes Xt to be\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;66;03m# of type float, hence the required cast back to int.\u001b[39;00m\n\u001b[0;32m--> 685\u001b[0m inv_transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCategorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inv_transform, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    687\u001b[0m     inv_transform \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(inv_transform)\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/skopt/space/space.py:168\u001b[0m, in \u001b[0;36mDimension.inverse_transform\u001b[0;34m(self, Xt)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minverse_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, Xt):\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Inverse transform samples from the warped space back into the\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m       original space.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/skopt/space/transformers.py:309\u001b[0m, in \u001b[0;36mPipeline.inverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minverse_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m transformer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformers[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 309\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/skopt/space/transformers.py:275\u001b[0m, in \u001b[0;36mNormalize.inverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    273\u001b[0m X_orig \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhigh \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_int:\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mround(X_orig)\u001b[38;5;241m.\u001b[39mastype(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_orig\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=19.68332293007883, penalty=l1, solver=saga;, score=0.627 total time=   0.1s\n",
      "[CV 3/5] END C=19.68332293007883, penalty=l1, solver=saga;, score=0.765 total time=   0.0s\n",
      "[CV 4/5] END C=19.68332293007883, penalty=l1, solver=saga;, score=0.840 total time=   0.0s\n",
      "[CV 5/5] END C=19.68332293007883, penalty=l1, solver=saga;, score=0.720 total time=   0.1s\n",
      "[CV 1/5] END C=18.43267706895152, penalty=l2, solver=newton-cg;, score=0.824 total time=   0.0s\n",
      "[CV 2/5] END C=18.43267706895152, penalty=l2, solver=newton-cg;, score=0.804 total time=   0.0s\n",
      "[CV 3/5] END C=18.43267706895152, penalty=l2, solver=newton-cg;, score=0.863 total time=   0.0s\n",
      "[CV 4/5] END C=18.43267706895152, penalty=l2, solver=newton-cg;, score=0.840 total time=   0.0s\n",
      "[CV 4/5] END C=5.295562742413681, penalty=l2, solver=lbfgs;, score=0.820 total time=   0.1s\n",
      "[CV 5/5] END C=5.295562742413681, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=3.6450290122373095, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=3.6450290122373095, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=3.6450290122373095, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=3.6450290122373095, penalty=l2, solver=lbfgs;, score=0.820 total time=   0.1s\n",
      "[CV 5/5] END C=3.6450290122373095, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=13.92397193076634, penalty=l2, solver=sag;, score=0.745 total time=   0.0s\n",
      "[CV 2/5] END C=11.351151433346887, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=11.351151433346887, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=11.351151433346887, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=11.351151433346887, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=1.8316225168969191, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=1.8316225168969191, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=1.8316225168969191, penalty=l2, solver=lbfgs;, score=0.882 total time=   0.1s\n",
      "[CV 4/5] END C=1.8316225168969191, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 4/5] END C=19.843817698674314, penalty=l1, solver=saga;, score=0.840 total time=   0.0s\n",
      "[CV 5/5] END C=19.843817698674314, penalty=l1, solver=saga;, score=0.720 total time=   0.0s\n",
      "[CV 1/5] END C=2.819384259448111, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=2.819384259448111, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=2.819384259448111, penalty=l2, solver=lbfgs;, score=0.882 total time=   0.1s\n",
      "[CV 4/5] END C=2.819384259448111, penalty=l2, solver=lbfgs;, score=0.820 total time=   0.1s\n",
      "[CV 5/5] END C=2.819384259448111, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=6.724711241922247, penalty=l1, solver=saga;, score=0.706 total time=   0.0s\n",
      "[CV 4/5] END C=15.46025762818316, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=15.46025762818316, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=2.950315177972523, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=2.950315177972523, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=2.950315177972523, penalty=l2, solver=lbfgs;, score=0.882 total time=   0.1s\n",
      "[CV 4/5] END C=2.950315177972523, penalty=l2, solver=lbfgs;, score=0.820 total time=   0.1s\n",
      "[CV 5/5] END C=2.950315177972523, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=5.734840030268632, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 4/5] END C=18.026252988717314, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=18.026252988717314, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=3.010625846470093, penalty=l2, solver=liblinear;, score=0.804 total time=   0.0s\n",
      "[CV 2/5] END C=3.010625846470093, penalty=l2, solver=liblinear;, score=0.824 total time=   0.0s\n",
      "[CV 3/5] END C=3.010625846470093, penalty=l2, solver=liblinear;, score=0.902 total time=   0.0s\n",
      "[CV 4/5] END C=3.010625846470093, penalty=l2, solver=liblinear;, score=0.860 total time=   0.0s\n",
      "[CV 5/5] END C=3.010625846470093, penalty=l2, solver=liblinear;, score=0.820 total time=   0.0s\n",
      "[CV 1/5] END C=15.46025762818316, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=15.46025762818316, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=15.46025762818316, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 3/5] END C=19.637222882018694, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 5/5] END C=19.637222882018694, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=15.284470080968969, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 3/5] END C=15.284470080968969, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 5/5] END C=15.284470080968969, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END C=17.2941310535965, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=17.2941310535965, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=3.0172670587073545, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=3.0172670587073545, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=3.0172670587073545, penalty=l2, solver=lbfgs;, score=0.882 total time=   0.1s\n",
      "[CV 4/5] END C=3.0172670587073545, penalty=l2, solver=lbfgs;, score=0.820 total time=   0.1s\n",
      "[CV 5/5] END C=3.0172670587073545, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END C=4.869394262456313, penalty=l2, solver=lbfgs;, score=0.820 total time=   0.1s\n",
      "[CV 5/5] END C=4.869394262456313, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=5.351075090971959, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=5.351075090971959, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=5.351075090971959, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=5.351075090971959, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=5.351075090971959, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=11.351151433346887, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 5/5] END C=1.8316225168969191, penalty=l2, solver=lbfgs;, score=0.820 total time=   0.1s\n",
      "[CV 1/5] END C=11.126633808005367, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=11.126633808005367, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=11.126633808005367, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=11.126633808005367, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=11.126633808005367, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=14.294897319340754, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=14.294897319340754, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 2/5] END C=6.724711241922247, penalty=l1, solver=saga;, score=0.627 total time=   0.0s\n",
      "[CV 3/5] END C=6.724711241922247, penalty=l1, solver=saga;, score=0.765 total time=   0.0s\n",
      "[CV 4/5] END C=6.724711241922247, penalty=l1, solver=saga;, score=0.840 total time=   0.0s\n",
      "[CV 5/5] END C=6.724711241922247, penalty=l1, solver=saga;, score=0.720 total time=   0.0s\n",
      "[CV 1/5] END C=1.3429529621898395, penalty=l1, solver=saga;, score=0.706 total time=   0.0s\n",
      "[CV 2/5] END C=1.3429529621898395, penalty=l1, solver=saga;, score=0.627 total time=   0.0s\n",
      "[CV 3/5] END C=1.3429529621898395, penalty=l1, solver=saga;, score=0.765 total time=   0.0s\n",
      "[CV 4/5] END C=1.3429529621898395, penalty=l1, solver=saga;, score=0.840 total time=   0.1s\n",
      "[CV 3/5] END C=11.68306594743975, penalty=l2, solver=newton-cg;, score=0.863 total time=   0.0s\n",
      "[CV 4/5] END C=11.68306594743975, penalty=l2, solver=newton-cg;, score=0.840 total time=   0.0s\n",
      "[CV 5/5] END C=11.68306594743975, penalty=l2, solver=newton-cg;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=6.235611304225278, penalty=l2, solver=newton-cg;, score=0.804 total time=   0.0s\n",
      "[CV 2/5] END C=6.235611304225278, penalty=l2, solver=newton-cg;, score=0.784 total time=   0.0s\n",
      "[CV 3/5] END C=6.235611304225278, penalty=l2, solver=newton-cg;, score=0.863 total time=   0.0s\n",
      "[CV 4/5] END C=6.235611304225278, penalty=l2, solver=newton-cg;, score=0.840 total time=   0.0s\n",
      "[CV 5/5] END C=6.235611304225278, penalty=l2, solver=newton-cg;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END C=7.548110444125139, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=3.107850864263216, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=3.107850864263216, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=3.107850864263216, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=3.107850864263216, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=3.107850864263216, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=19.637222882018694, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=19.637222882018694, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 4/5] END C=19.637222882018694, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 2/5] END C=15.284470080968969, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 4/5] END C=15.284470080968969, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 4/5] END C=4.781548015823157, penalty=l2, solver=sag;, score=0.860 total time=   0.0s\n",
      "[CV 5/5] END C=4.781548015823157, penalty=l2, solver=sag;, score=0.760 total time=   0.0s\n",
      "[CV 1/5] END C=17.2941310535965, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=17.2941310535965, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 5/5] END C=18.770150033231992, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=16.65565483431463, penalty=l2, solver=liblinear;, score=0.804 total time=   0.0s\n",
      "[CV 2/5] END C=16.65565483431463, penalty=l2, solver=liblinear;, score=0.824 total time=   0.0s\n",
      "[CV 3/5] END C=16.65565483431463, penalty=l2, solver=liblinear;, score=0.882 total time=   0.0s\n",
      "[CV 4/5] END C=16.65565483431463, penalty=l2, solver=liblinear;, score=0.840 total time=   0.0s\n",
      "[CV 5/5] END C=16.65565483431463, penalty=l2, solver=liblinear;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=2.61490894253778, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=2.61490894253778, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END C=8.674188293965093, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=8.674188293965093, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=8.674188293965093, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=8.674188293965093, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=8.674188293965093, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=4.869394262456313, penalty=l2, solver=lbfgs;, score=0.804 total time=   0.1s\n",
      "[CV 2/5] END C=4.869394262456313, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=4.869394262456313, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 1/5] END C=15.016988344476015, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=15.016988344476015, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=15.016988344476015, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=15.016988344476015, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=15.016988344476015, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=19.843817698674314, penalty=l1, solver=saga;, score=0.706 total time=   0.0s\n",
      "[CV 2/5] END C=19.843817698674314, penalty=l1, solver=saga;, score=0.627 total time=   0.0s\n",
      "[CV 3/5] END C=19.843817698674314, penalty=l1, solver=saga;, score=0.765 total time=   0.0s\n",
      "[CV 5/5] END C=1.3429529621898395, penalty=l1, solver=saga;, score=0.720 total time=   0.0s\n",
      "[CV 1/5] END C=17.381946667309446, penalty=l2, solver=liblinear;, score=0.804 total time=   0.0s\n",
      "[CV 2/5] END C=17.381946667309446, penalty=l2, solver=liblinear;, score=0.824 total time=   0.0s\n",
      "[CV 3/5] END C=17.381946667309446, penalty=l2, solver=liblinear;, score=0.882 total time=   0.0s\n",
      "[CV 4/5] END C=17.381946667309446, penalty=l2, solver=liblinear;, score=0.840 total time=   0.0s\n",
      "[CV 5/5] END C=17.381946667309446, penalty=l2, solver=liblinear;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=11.68306594743975, penalty=l2, solver=newton-cg;, score=0.824 total time=   0.0s\n",
      "[CV 2/5] END C=11.68306594743975, penalty=l2, solver=newton-cg;, score=0.784 total time=   0.0s\n",
      "[CV 2/5] END C=5.734840030268632, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=5.734840030268632, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=5.734840030268632, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n",
      "[CV 5/5] END C=5.734840030268632, penalty=l2, solver=lbfgs;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END C=7.548110444125139, penalty=l2, solver=lbfgs;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END C=7.548110444125139, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=7.548110444125139, penalty=l2, solver=lbfgs;, score=0.863 total time=   0.1s\n",
      "[CV 4/5] END C=7.548110444125139, penalty=l2, solver=lbfgs;, score=0.840 total time=   0.1s\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real,Categorical\n",
    "\n",
    "# The base model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# The parameters\n",
    "parameters = [{\"C\":Real(0.01,20,prior=\"uniform\"), #Instead of uniform, we now need to use Real with prior uniform\n",
    "              \"solver\":Categorical([\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"])}] \n",
    "                      # Instead of normal list, we now need to wrap it into a Categorical\n",
    "    \n",
    "# Number of searches our model is allowed to do\n",
    "n_iter_search = 100\n",
    "\n",
    "# The bayes_search 'model' similar to grid and random search\n",
    "bayes_search = BayesSearchCV(estimator=model,\n",
    "                                   cv=5,\n",
    "                                   search_spaces=parameters,\n",
    "                                   n_iter=n_iter_search,\n",
    "                                   n_jobs=-1,\n",
    "                                   verbose=5)\n",
    "# Fit on the training data\n",
    "bayes_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BayesSearchCV' object has no attribute 'scorer_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Print the sores of the best model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore of best model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbayes_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Predict the classes of test set\u001b[39;00m\n\u001b[1;32m      5\u001b[0m y_pred\u001b[38;5;241m=\u001b[39mbayes_search\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/model_selection/_search.py:437\u001b[0m, in \u001b[0;36mBaseSearchCV.score\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    435\u001b[0m _check_refit(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    436\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer_\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo score function explicitly defined, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the estimator doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt provide one \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscorer_, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BayesSearchCV' object has no attribute 'scorer_'"
     ]
    }
   ],
   "source": [
    "# Print the sores of the best model\n",
    "print(f\"Score of best model: {bayes_search.score(X_test,y_test)}\")\n",
    "\n",
    "# Predict the classes of test set\n",
    "y_pred=bayes_search.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "#Print the best parameters\n",
    "print(\"\\nBest parameters:\")\n",
    "print(bayes_search.best_params_)\n",
    "print(bayes_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BayesSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m roc_auc_grid \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mauc(fpr_grid, tpr_grid)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Do exactly the same for the bayes search model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m probs_bayes \u001b[38;5;241m=\u001b[39m \u001b[43mbayes_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m preds_bayes \u001b[38;5;241m=\u001b[39m probs_bayes[:,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     18\u001b[0m fpr_bayes, tpr_bayes, threshold_bayes \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mroc_curve(y_test, preds_bayes)\n",
      "File \u001b[0;32m~/miniconda3/envs/bit07/lib/python3.9/site-packages/sklearn/model_selection/_search.py:522\u001b[0m, in \u001b[0;36mBaseSearchCV.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call predict_proba on the estimator with the best found parameters.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \n\u001b[1;32m    505\u001b[0m \u001b[38;5;124;03mOnly available if ``refit=True`` and the underlying estimator supports\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03m    to that in the fitted attribute :term:`classes_`.\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    521\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BayesSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "# ROC\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "\n",
    "# Get the probabilities for the grid search from earlier on\n",
    "probs_grid = grid_search.predict_proba(X_test)\n",
    "# Keep only those of the 'true' class, so the second column: [:,1] => all rows, column 1 (zero based so actually second col)\n",
    "preds_grid = probs_grid[:,1]\n",
    "# Calculate false and true positives using this information\n",
    "fpr_grid, tpr_grid, threshold_grid = metrics.roc_curve(y_test, preds_grid)\n",
    "# Calculare Area Under the Curve (AUC)\n",
    "roc_auc_grid = metrics.auc(fpr_grid, tpr_grid)\n",
    "\n",
    "# Do exactly the same for the bayes search model\n",
    "probs_bayes = bayes_search.predict_proba(X_test)\n",
    "preds_bayes = probs_bayes[:,1]\n",
    "fpr_bayes, tpr_bayes, threshold_bayes = metrics.roc_curve(y_test, preds_bayes)\n",
    "roc_auc_bayes = metrics.auc(fpr_bayes, tpr_bayes)\n",
    "\n",
    "# plot the ROC curve, also add a legend, titles and a diagonal dotted line representing 50/50 chance\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr_grid, tpr_grid, 'b', alpha=0.5, label = 'Grid AUROC = %0.2f' % roc_auc_grid)\n",
    "plt.plot(fpr_bayes, tpr_bayes, 'r', alpha=0.5, label = 'Bayes AUROC = %0.2f' % roc_auc_bayes)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a936e2f7d0e946a9b7a4d77e64830f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='Treshold', max=1.0, step=0.01), Output()), _dom_clas"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive ROC\n",
    "# This roc will show you where we land on the graph if we use a different treshold. So basically how this graph is\n",
    "# constructed.\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "Total_positives=sum(y_test)\n",
    "Total_negatives=sum([1 for x in y_test if x==0 ])\n",
    "\n",
    "@interact(Treshold=(0,1,0.01))\n",
    "def drawROC(Treshold=0.5):\n",
    "    probs_bayes = bayes_search.predict_proba(X_test)\n",
    "    preds_bayes = probs_bayes[:,1]\n",
    "    fpr_bayes, tpr_bayes, threshold_bayes = metrics.roc_curve(y_test, preds_bayes)\n",
    "    roc_auc_bayes = metrics.auc(fpr_bayes, tpr_bayes)\n",
    "    \n",
    "    pred_treshold = [0 if x<Treshold else 1 for x in probs_bayes[:,1]]\n",
    "    true_positives = sum([1 for x,y in zip(y_test,pred_treshold) if x==y==1])\n",
    "    true_negatives = sum([1 for x,y in zip(y_test,pred_treshold) if x==y==0])\n",
    "    \n",
    "    print(\"TPR: \",tpr:=true_positives/Total_positives)\n",
    "    print(\"FPR: \",fpr:=1-true_negatives/Total_negatives)\n",
    "    # plot the curve\n",
    "    \n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr_bayes, tpr_bayes, 'r', alpha=0.5, label = 'Bayes AUC = %0.2f' % roc_auc_bayes)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.scatter(fpr,tpr)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
